{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download dataset from: https://drive.google.com/uc?id=1-WZKE5xHw-3m_SL_PtOgwkzdFROIWqih"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"max_colwidth\", 800)\n",
    "\n",
    "raw_df = pd.read_csv(\"data.csv\")\n",
    "raw_df = raw_df[raw_df[\"Score\"] > 0]\n",
    "df = raw_df[:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def preprocessLemmatize(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    taggedTokens = nltk.pos_tag(tokens)\n",
    "    tagMap = defaultdict(lambda: wordnet.NOUN)\n",
    "    tagMap.update({\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"J\": wordnet.ADJ, \"R\": wordnet.ADV})\n",
    "    lemmatizedTokens = []\n",
    "    for token, tag in taggedTokens:\n",
    "        pos = tagMap.get(tag[0], wordnet.NOUN)\n",
    "        lemmatizedTokens.append(lemmatizer.lemmatize(token, pos))\n",
    "    return lemmatizedTokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Iterable\n",
    "\n",
    "\n",
    "def predict(\n",
    "    text: str,\n",
    "    preprocessFunction: Callable = preprocessLemmatize,\n",
    "    goodWords: Iterable | None = None,\n",
    "    badWords: Iterable | None = None,\n",
    ") -> int:\n",
    "    defaultGoodWords = {\"good\", \"like\", \"love\", \"great\", \"amazing\"}\n",
    "    defaultBadWords = {\"bad\", \"hate\", \"horrible\", \"terrible\", \"awful\"}\n",
    "\n",
    "    goodWords = goodWords or defaultGoodWords\n",
    "    badWords = badWords or defaultBadWords\n",
    "\n",
    "\n",
    "    tokens = preprocessFunction(text)\n",
    "\n",
    "    sentiment = 0  # neutral sentiment by default\n",
    "    for token in tokens:\n",
    "        if token in goodWords:\n",
    "            sentiment += 1\n",
    "        elif token in badWords:\n",
    "            sentiment -= 1\n",
    "\n",
    "    return sentiment\n",
    "\n",
    "\n",
    "def predictStars(\n",
    "    text,\n",
    "    preprocessFunction: Callable = preprocessLemmatize,\n",
    "    goodWords: Iterable | None = None,\n",
    "    badWords: Iterable | None = None,\n",
    "):\n",
    "    sentiment = predict(text, preprocessFunction, goodWords, badWords)\n",
    "    if sentiment > 1:\n",
    "        return 5\n",
    "    elif sentiment == 1:\n",
    "        return 4\n",
    "    elif sentiment == 0:\n",
    "        return 3\n",
    "    elif sentiment == -1:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning based Model\n",
    "Instead of the programmer picking the \"good\" and \"bad\" tokens, we let the program create its own set of \"good\" and \"bad\" tokens by training on the reviews. The tokens that appear the most in 5-star reviews have a high chance of being the \"good\" tokens and similarly, the tokens that appear the most in 1-star reviews have a high chance of being the \"bad\" tokens. Of course, it is not that simple, but we'll get to the challenging part in a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "fiveStarTokenCounter = Counter()\n",
    "oneStarTokenCounter = Counter()\n",
    "allTokenCounter = Counter()\n",
    "OFFSET = 10\n",
    "\n",
    "for text, score in zip(df[\"Text\"], df[\"Score\"]):\n",
    "    tokens = preprocessLemmatize(text)\n",
    "    if score == 5:\n",
    "        fiveStarTokenCounter.update(tokens)\n",
    "    elif score == 1:\n",
    "        oneStarTokenCounter.update(tokens)\n",
    "    allTokenCounter.update(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Good tokens:\")\n",
    "for token, count in fiveStarTokenCounter.most_common(10):\n",
    "    print(f\"{token} ({count})\", end=\", \")\n",
    "\n",
    "print(\"\\nBad tokens:\")\n",
    "for token, count in oneStarTokenCounter.most_common(10):\n",
    "    print(f\"{token} ({count})\", end=\", \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Challenging Part\n",
    "The \"good\" and \"bad\" tokens found using taking the most common words in 5-star and 1-star reviews happen to the be the most common words you'd find in any review (Duh!!).<br>\n",
    "To overcome this issue, we must come up with a way to penalize the tokens that just occur in all reviews and not only in the 5-star and 1-star reviews we're concerned about. We can do so by divding the frequency of the token in 5-star or 1-star reviews by its total frequency in all reviews.<br>\n",
    "But this also means, tokens that are really unique like an ingredient's name that may appear only few times will be ranked high (the denominator will be low). We definitely dont want that. To fix this, we'll assume that we've seen every word at least `OFFSET` number of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiveStarNormalized = {\n",
    "    token: count / (allTokenCounter[token] + OFFSET)\n",
    "    for token, count in fiveStarTokenCounter.items()\n",
    "}\n",
    "sortedFiveStarNormalized = sorted(fiveStarNormalized.items(), key=lambda x: x[1], reverse=True)\n",
    "goodWords = [token for token, _ in sortedFiveStarNormalized[:10]]\n",
    "\n",
    "\n",
    "oneStarNormalized = {\n",
    "    token: count / (allTokenCounter[token] + OFFSET) for token, count in oneStarTokenCounter.items()\n",
    "}\n",
    "sortedOneStarNormalized = sorted(oneStarNormalized.items(), key=lambda x: x[1], reverse=True)\n",
    "badWords = [token for token, _ in sortedOneStarNormalized[:10]]\n",
    "\n",
    "print(\"Good words:\", end=\" \")\n",
    "print(*goodWords, sep=\", \")\n",
    "\n",
    "print(\"Bad words:\", end=\" \")\n",
    "print(*badWords, sep=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"good\" and \"bad\" tokens now are promising. Let's go ahead and predict the number of stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Prediction\"] = df[\"Text\"].apply(\n",
    "    lambda doc: predictStars(\n",
    "        doc,\n",
    "        goodWords=goodWords,\n",
    "        badWords=badWords,\n",
    "    )\n",
    ")\n",
    "score = sum(df[\"Prediction\"] == df[\"Score\"]) / len(df)\n",
    "print(\"Accuracy:\", round(100 * score, 3), \"%\")\n",
    "df[[\"Summary\", \"Text\", \"Score\", \"Prediction\"]].head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "Vectorization is a methodology in NLP to map words or phrases from vocabulary to a corresponding vector of real numbers. By vectorizing the documents, we can convert the document from a sequence of tokens (strings) to an array of numbers which will be used in regression models.<br>\n",
    "It is to be noted that the order in which the tokens occured is lost by vectorizing the document. But since we didn't care about order to begin with, it should be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"You like dogs\",\n",
    "    \"You like cats\",\n",
    "    \"You love dogs\",\n",
    "    \"You love cats\",\n",
    "]\n",
    "\n",
    "vocab = set()\n",
    "for doc in corpus:\n",
    "    tokens = preprocessLemmatize(doc)\n",
    "    vocab.update(tokens)\n",
    "\n",
    "vocab = sorted(vocab)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordToIndex = {word: index for index, word in enumerate(vocab)}\n",
    "\n",
    "vectors = []\n",
    "for doc in corpus:\n",
    "    tokens = preprocessLemmatize(doc)\n",
    "    vector = [0] * len(vocab)\n",
    "    for token in tokens:\n",
    "        index = wordToIndex[token]\n",
    "        vector[index] += 1\n",
    "    vectors.append(vector)\n",
    "\n",
    "print(*vectors, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectors = vectorizer.fit_transform(corpus)\n",
    "print(vectors.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_df[:10000]\n",
    "\n",
    "train_df = df[:int(len(df) * 0.9)]\n",
    "test_df = df[int(len(df) * 0.9):]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "train_features = vectorizer.fit_transform(train_df[\"Text\"])\n",
    "test_features = vectorizer.transform(test_df[\"Text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "Oooh, finally to the machine learning part. Logistic regression is one of the most popular Machine Learning algorithms, which comes under the Supervised Learning technique. It is used for predicting the categorical dependent variable using a given set of independent variables.<br>\n",
    "Logistic regression predicts the output of a categorical dependent variable. Therefore the outcome must be a categorical or discrete value. It can be either Yes or No, 0 or 1, true or False, etc. but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_classifier = LogisticRegression()\n",
    "lr_classifier.fit(train_features, train_df[\"Score\"])\n",
    "\n",
    "score = lr_classifier.score(test_features, test_df[\"Score\"])\n",
    "print(\"Accuracy:\", round(100 * score, 3), \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9809e28c19dfb975224638f8b8ef0f584d3176802c0e325eace288af197b9ed5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
