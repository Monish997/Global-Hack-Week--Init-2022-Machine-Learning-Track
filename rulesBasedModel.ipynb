{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules-based Model\n",
    "A very simple rules-based model is to compare the tokens in the document with a set of \"good\" and \"bad\" tokens that is picked by the programmer. The sentiment is positive if number of \"good\" tokens is more than number of \"bad\" tokens, negative if number of \"bad\" tokens is more than number of \"good\" token, neutral if number of \"good\" tokens equals number of \"bad\" tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Iterable\n",
    "\n",
    "\n",
    "def predict(\n",
    "    text,\n",
    "    preprocessFunction: Callable = str.split,\n",
    "    goodWords: Iterable | None = None,\n",
    "    badWords: Iterable | None = None,\n",
    ") -> int:\n",
    "    defaultGoodWords = {\"good\", \"like\", \"love\", \"great\", \"amazing\"}\n",
    "    defaultBadWords = {\"bad\", \"hate\", \"horrible\", \"terrible\", \"awful\"}\n",
    "\n",
    "    goodWords = goodWords or defaultGoodWords\n",
    "    badWords = badWords or defaultBadWords\n",
    "\n",
    "    tokens = preprocessFunction(text)\n",
    "\n",
    "    # sentiment = 0 => neutral\n",
    "    # sentiment > 0 => positive\n",
    "    # sentiment < 0 => negative\n",
    "\n",
    "    sentiment = 0  # neutral sentiment by default\n",
    "    for token in tokens:\n",
    "        if token in goodWords:\n",
    "            sentiment += 1\n",
    "        elif token in badWords:\n",
    "            sentiment -= 1\n",
    "\n",
    "    return sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 (Basic Tokenizer, NO POS Tagging, NO Lemmatization)\n",
    "# POS stands for Part of Speech like Noun, Verb, Adjective, Adverb\n",
    "# Lemmatization is the process of finding the base form of a word.\n",
    "# For example, the word \"like\" is the base form of \"likes\" and \"liked\".\n",
    "\n",
    "corpus = [\n",
    "    \"A. R. Rahman is a good film composer and songwriter.\",\n",
    "    \"Pineapple on pizzas tastes very bad.\",\n",
    "    \"He likes anime. Steins;Gate is his favourite\",\n",
    "    \"My introvert friend is TERRIBLE at communicating.\",\n",
    "]\n",
    "\n",
    "for doc in corpus:\n",
    "    print(doc)\n",
    "    sentiment = predict(doc)\n",
    "    if sentiment > 0:\n",
    "        print(\"Positive\")\n",
    "    elif sentiment < 0:\n",
    "        print(\"Negative\")\n",
    "    else:\n",
    "        print(\"Neutral\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2 (NLTK punkt tokenizer, NO POS Tagging, NO Lemmatization)\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "def preprocessPunkt(text):\n",
    "    return word_tokenize(text.lower())\n",
    "\n",
    "\n",
    "for doc in corpus:\n",
    "    print(doc)\n",
    "    print(\"Tokenized:\", preprocessPunkt(doc))\n",
    "    sentiment = predict(doc, preprocessPunkt)\n",
    "    if sentiment > 0:\n",
    "        print(\"Positive\")\n",
    "    elif sentiment < 0:\n",
    "        print(\"Negative\")\n",
    "    else:\n",
    "        print(\"Neutral\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3 (NLTP punkt tokenizer, POS Tagging, Lemmatization using WordNet)\n",
    "# WordNet is a lexical database of English. It is used to find the base form of a word.\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def preprocessLemmatize(text) -> list[str]:\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    taggedTokens = nltk.pos_tag(tokens)\n",
    "    tagMap = {\"N\": \"n\", \"V\": \"v\", \"J\": \"a\", \"R\": \"r\"}\n",
    "    lemmatizedTokens = []\n",
    "    for token, tag in taggedTokens:\n",
    "        pos = tagMap.get(tag[0], \"n\")\n",
    "        lemmatizedTokens.append(lemmatizer.lemmatize(token, pos))\n",
    "    return lemmatizedTokens\n",
    "\n",
    "\n",
    "for doc in corpus:\n",
    "    print(doc)\n",
    "    print(\"Lemmatized Tokens:\", preprocessLemmatize(doc))\n",
    "    sentiment = predict(doc, preprocessLemmatize)\n",
    "    if sentiment > 0:\n",
    "        print(\"Positive\")\n",
    "    elif sentiment < 0:\n",
    "        print(\"Negative\")\n",
    "    else:\n",
    "        print(\"Neutral\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Real-World Data (Amazon reviews)\n",
    "\n",
    "Download from: https://drive.google.com/uc?id=1-WZKE5xHw-3m_SL_PtOgwkzdFROIWqih\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"max_colwidth\", 800)\n",
    "\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "df = df[df[\"Score\"] > 1][:1000]\n",
    "df[[\"Summary\", \"Text\", \"Score\"]].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictStars(\n",
    "    text,\n",
    "    preprocessFunction=preprocessLemmatize,\n",
    "    goodWords: Iterable | None = None,\n",
    "    badWords: Iterable | None = None,\n",
    "):\n",
    "    sentiment = predict(text, preprocessFunction, goodWords, badWords)\n",
    "    if sentiment > 1:\n",
    "        return 5\n",
    "    elif sentiment == 1:\n",
    "        return 4\n",
    "    elif sentiment == 0:\n",
    "        return 3\n",
    "    elif sentiment == -1:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "df[\"Prediction\"] = df[\"Text\"].apply(predictStars)\n",
    "score = sum(df[\"Prediction\"] == df[\"Score\"]) / len(df)\n",
    "print(\"Accuracy:\", round(100 * score, 3), \"%\")\n",
    "df[[\"Summary\", \"Text\", \"Score\", \"Prediction\"]].head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9809e28c19dfb975224638f8b8ef0f584d3176802c0e325eace288af197b9ed5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
